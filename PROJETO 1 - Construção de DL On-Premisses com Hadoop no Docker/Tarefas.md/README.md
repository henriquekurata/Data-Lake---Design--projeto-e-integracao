# üöÄ ***Configura√ß√£o de ambiente Hadoop com Docker***

## **Descri√ß√£o do Projeto:**
Este projeto visa a configura√ß√£o de um ambiente Hadoop utilizando Docker para facilitar a implementa√ß√£o e o gerenciamento de clusters Hadoop em um ambiente controlado e isolado. O Hadoop √© um framework de c√≥digo aberto que permite o processamento e armazenamento de grandes volumes de dados de maneira distribu√≠da. Utilizando Docker, √© poss√≠vel criar containers que emulam a funcionalidade de n√≥s no cluster Hadoop, simplificando a configura√ß√£o e a gest√£o dos servi√ßos Hadoop, como o NameNode e DataNodes.

## **Objetivos:**
- **Configurar um ambiente Hadoop**: Instalar e configurar o Hadoop e JDK dentro de containers Docker.
- **Gerenciar o cluster Hadoop**: Configurar o NameNode e DataNodes para processar e armazenar dados de forma distribu√≠da.
- **Automatizar a configura√ß√£o**: Utilizar Docker para criar, ajustar e gerenciar containers de forma eficiente.

## üõ†Ô∏è **Ferramentas Utilizadas**
**Docker**: Plataforma para criar, implantar e gerenciar containers, facilitando a configura√ß√£o e o isolamento de ambientes.
**Hadoop**: Framework para processamento e armazenamento de grandes volumes de dados, com suporte para armazenamento distribu√≠do e processamento paralelo.

## üìã **Descri√ß√£o do Processo**

1. **Download e Prepara√ß√£o**:
   - Baixe o Apache Hadoop e o JDK 8.
   - Organize os arquivos em pastas adequadas e descompacte-os.
2. **Cria√ß√£o dos Containers**:
   - Configure o Dockerfile para o NameNode e DataNodes.
   - Construa e execute os containers com as configura√ß√µes apropriadas.
3. **Configura√ß√£o e Inicializa√ß√£o**:
   - Configure a rede Docker necess√°ria para os containers.
   - Inicialize e configure o NameNode e DataNodes, ajustando permiss√µes e configurando as chaves SSH.
4. **Verifica√ß√£o e Acesso**:
   - Acesse o painel de gest√£o do Hadoop via navegador para monitorar o estado do cluster.


## üñ•Ô∏è Comandos:

### Prepara√ß√£o do NameNode

1. **Download e Configura√ß√£o**:
   - Baixe o Apache Hadoop e o JDK 8.
   - Coloque os arquivos na pasta "binarios", descompacte-os e renomeie as pastas para "hadoop" e "jdk".
   
   **Obs:** Fa√ßa o download diretamente na documenta√ß√£o do Hadoop e Java.

   - Abra o terminal ou prompt de comando, navegue at√© a pasta do NameNode e execute o comando para criar a imagem:
   - 
     docker build . -t namenode:dsa
     

2. **Dockerfile para o NameNode**:

```
#https://hub.docker.com/_/ubuntu
FROM ubuntu:latest

#Updates e instala√ß√µes
RUN \
  apt-get update && apt-get install -y \
  openssh-server \
  python3 \
  rsync \
  sudo \
  arp-scan \
  net-tools \
  iputils-ping \
  vim \
  && apt-get clean

#Cria usu√°rio para a instala√ß√£o do Hadoop
RUN useradd -m hduser && echo "hduser:supergroup" | chpasswd && adduser hduser sudo && echo "hduser ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers && cd /usr/bin/ && sudo ln -s python3 python

#Copia o arquivo de configura√ß√£o do ssh
ADD ./config-files/ssh_config /etc/ssh/ssh_config

#Muda o usu√°rio
USER hduser

#Pasta de trabalho
WORKDIR /home/hduser

#Cria a chave ssh
RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && chmod 0600 ~/.ssh/authorized_keys

#Usu√°rio de trabalho
ENV HDFS_NAMENODE_USER=hduser
ENV HDFS_DATANODE_USER=hduser
ENV HDFS_SECONDARYNAMENODE_USER=hduser
ENV YARN_RESOURCEMANAGER_USER=hduser
ENV YARN_NODEMANAGER_USER=hduser

#Copia os bin√°rios do JDK
ADD ./binarios/jdk ./jdk

#Vari√°veis de ambiente JDK
ENV JAVA_HOME=/home/hduser/jdk
ENV PATH=$PATH:$JAVA_HOME:$JAVA_HOME/bin

#Copia os bin√°rios do Hadoop
ADD ./binarios/hadoop ./hadoop

#Vari√°veis de ambiente do Hadoop
ENV HADOOP_HOME=/home/hduser/hadoop
ENV PATH=$PATH:$HADOOP_HOME
ENV PATH=$PATH:$HADOOP_HOME/bin
ENV PATH=$PATH:$HADOOP_HOME/sbin

#Pastas para os arquivos do NameNode
RUN mkdir /home/hduser/hdfs
RUN mkdir /home/hduser/hdfs/namenode

#Vari√°veis de ambiente
RUN echo "PATH=$PATH:$JAVA_HOME/bin" >> ~/.bashrc
RUN echo "PATH=$PATH:$HADOOP_HOME/bin" >> ~/.bashrc
RUN echo "PATH=$PATH:$HADOOP_HOME/sbin" >> ~/.bashrc

#Copia os arquivos de configura√ß√£o
ADD ./config-files/hadoop-env.sh $HADOOP_HOME/etc/hadoop/
ADD ./config-files/core-site.xml $HADOOP_HOME/etc/hadoop/
ADD ./config-files/hdfs-site.xml $HADOOP_HOME/etc/hadoop/
ADD ./config-files/workers $HADOOP_HOME/etc/hadoop/

#Portas que poder√£o ser usadas
EXPOSE 50070 50075 50010 50020 50090 8020 9000 9864 9870 8030 8031 8032 8033 8040 8042 22

``` 

#### Documenta√ß√£o do `docker build`:
https://docs.docker.com/engine/reference/commandline/build/

3. **Precisaremos de uma rede. Verifique as redes dispon√≠veis e ent√£o crie uma com as instru√ß√µes abaixo:**

docker network ls

docker network create -d bridge dsa_dl_net

4. **Crie e inicialize o container com a instru√ß√£o abaixo:**

docker run -it -d --net dsa_dl_net --hostname hdpmaster -p 9870:9870 -p 50030:50030 -p 8020:8020 --name namenode namenode:dsa 

#Documenta√ß√£o do docker run:
https://docs.docker.com/engine/reference/commandline/run/

5. **Acesse o container usando a CLI no Docker Desktop e execute os seguintes comandos:**

#Restart do servi√ßo ssh

sudo service ssh restart

#Ajuste dos privil√©gios

sudo chown -R hduser:hduser /home/hduser/jdk

sudo chown -R hduser:hduser /home/hduser/hadoop

#Formate o NameNode (somente na primeira execu√ß√£o)

hdfs namenode -format

#Start do servi√ßo do NameNode

hdfs --daemon start namenode

#Se precisar parar o servi√ßo:

hdfs --daemon stop namenode

#Acesse: http://localhost:9870/

#### Obs: Para mais informa√ß√µes sobre os arquivos de configura√ß√µes do hadoop, basta acessar o site http://apache.github.io/hadoop/ na aba configuration

----

### Prepara√ß√£o dos DataNodes

1. **Fa√ßa o download do Apache Hadoop e do JDK 8, coloque na pasta "binarios", descompacte os arquivos e renomeie as pastas. O procedimento √© o mesmo do namenode.**

2. **Abra o terminal ou prompt de comando, navegue at√© a pasta do DataNode e execute a instru√ß√£o abaixo para criar a imagem:**
docker build . -t datanode:dsa

#Arquivo de Configura√ß√£o do DataNode no Cluster HDFS

#O sistema operacional ser√° o Ubuntu na vers√£o mais atual

#https://hub.docker.com/_/ubuntu
FROM ubuntu:latest

```
#Updates e instala√ß√µes
RUN \
  apt-get update && apt-get install -y \
  openssh-server \
  python3 \
  rsync \
  sudo \
  arp-scan \
  net-tools \
  iputils-ping \
  vim \
  && apt-get clean

#Cria usu√°rio para a instala√ß√£o do Hadoop
RUN useradd -m hduser && echo "hduser:supergroup" | chpasswd && adduser hduser sudo && echo "hduser ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers && cd /usr/bin/ && sudo ln -s python3 python

#Copia o arquivo de configura√ß√£o do ssh
ADD ./config-files/ssh_config /etc/ssh/ssh_config

#Muda o usu√°rio
USER hduser

#Pasta de trabalho
WORKDIR /home/hduser

#Usu√°rio de trabalho
ENV HDFS_NAMENODE_USER=hduser
ENV HDFS_DATANODE_USER=hduser
ENV HDFS_SECONDARYNAMENODE_USER=hduser
ENV YARN_RESOURCEMANAGER_USER=hduser
ENV YARN_NODEMANAGER_USER=hduser

#Copia os bin√°rios do JDK
ADD ./binarios/jdk ./jdk

#Vari√°veis de ambiente JDK
ENV JAVA_HOME=/home/hduser/jdk
ENV PATH=$PATH:$JAVA_HOME:$JAVA_HOME/bin

#Copia os bin√°rios do Hadoop
ADD ./binarios/hadoop ./hadoop

#Vari√°veis de ambiente do Hadoop
ENV HADOOP_HOME=/home/hduser/hadoop
ENV PATH=$PATH:$HADOOP_HOME
ENV PATH=$PATH:$HADOOP_HOME/bin
ENV PATH=$PATH:$HADOOP_HOME/sbin

#Pastas para os arquivos do DataNode
RUN mkdir /home/hduser/hdfs
RUN mkdir /home/hduser/hdfs/datanode

#Copia os arquivos de configura√ß√£o
ADD ./config-files/hadoop-env.sh $HADOOP_HOME/etc/hadoop/
ADD ./config-files/core-site.xml $HADOOP_HOME/etc/hadoop/
ADD ./config-files/hdfs-site.xml $HADOOP_HOME/etc/hadoop/
ADD ./config-files/workers $HADOOP_HOME/etc/hadoop/

#Portas que poder√£o ser usadas
EXPOSE 22

```

#Documenta√ß√£o do `docker build`:
https://docs.docker.com/engine/reference/commandline/build/

3. **Precisaremos de uma rede. Verifique se a rede `dsa_dl_net` criada no cap√≠tulo anterior est√° criada:**
docker network ls

4. **Crie e inicialize o container de cada datanode (criaremos 2) com cada instru√ß√£o abaixo:**
docker run -it -d --net dsa_dl_net --hostname datanode1 --name datanode1 datanode:dsa

docker run -it -d --net dsa_dl_net --hostname datanode2 --name datanode2 datanode:dsa

#Documenta√ß√£o do docker run:
https://docs.docker.com/engine/reference/commandline/run/

5. **Acesse cada container usando a CLI no Docker Desktop e execute as instru√ß√µes abaixo:**

#### Restart do servi√ßo ssh

sudo service ssh restart

#### Ajuste dos privil√©gios

sudo chown -R hduser:hduser /home/hduser/jdk

sudo chown -R hduser:hduser /home/hduser/hadoop

#### Crie a pasta ~/.ssh

mkdir ~/.ssh

#### Crie o arquivo ~/.ssh/authorized_keys

touch ~/.ssh/authorized_keys

#### Ajuste o privil√©gio

chmod 600 ~/.ssh/authorized_keys

#### Copie a chave que est√° em /home/hduser/.ssh/authorized_keys no NameNode para o mesmo arquivo em cada datanode.

####Start do servi√ßo do DataNode

hdfs --daemon start datanode

#### Se precisar parar o servi√ßo:

hdfs --daemon stop datanode

#### Acesse o painel de gest√£o pelo navegador

Obs: Se n√£o funcionar o endere√ßo 0.0.0.0 use localhost

Obs: Para inicializar o datanode √© necess√°rio limpar a pasta /home/hduser/hdfs/datanode/ (sudo rm -rf *)


---
## Contato

Se tiver d√∫vidas ou sugest√µes sobre o projeto, entre em contato comigo:

- üíº [LinkedIn](https://www.linkedin.com/in/henrique-k-32967a2b5/)
- üê± [GitHub](https://github.com/henriquekurata?tab=overview&from=2024-09-01&to=2024-09-01)