# üöÄ ***Processamento de Dados com Amazon EMR***

## **Descri√ß√£o do Projeto:**
Este projeto visa processar dados utilizando Amazon EMR (Elastic MapReduce) para executar um job PySpark. O objetivo √© configurar um cluster EMR, carregar e processar dados armazenados no S3 e realizar an√°lise com PySpark. O resultado √© salvo de volta no S3 para posterior an√°lise ou uso.

## **Funcionalidades Principais:**

- **Cria√ß√£o e Configura√ß√£o do Ambiente:**
  - Cria√ß√£o de buckets no Amazon S3 para armazenar dados e scripts.
  - Configura√ß√£o de um cluster EMR com Apache Spark para processamento de dados.

- **Carregamento e Processamento de Dados:**
  - Upload de dados e scripts PySpark para o bucket S3.
  - Execu√ß√£o de um job PySpark no cluster EMR para processar os dados.
  - O job PySpark l√™ os dados, realiza consultas e grava resultados processados de volta no S3.

- **An√°lise de Dados:**
  - O script PySpark realiza a an√°lise de dados para identificar os restaurantes com o maior n√∫mero de "red violations" (infra√ß√µes graves).
  - Gera√ß√£o de um DataFrame com os 10 principais restaurantes com mais infra√ß√µes vermelhas.

- **Acesso e Execu√ß√£o:**
  - Conex√£o ao cluster EMR via SSH.
  - Configura√ß√£o e execu√ß√£o de steps no cluster para processamento de dados usando o script PySpark.
  - Armazenamento dos resultados processados no S3 para f√°cil acesso e an√°lise posterior.

### **Resultado:**
Os dados processados s√£o salvos em um bucket S3, permitindo f√°cil acesso e an√°lise dos resultados obtidos pela execu√ß√£o do job PySpark.


## üõ†Ô∏è **Ferramentas Utilizadas**
- **AWS S3:** Servi√ßo de armazenamento de objetos para armazenar dados e scripts.
- **Amazon EMR:** Servi√ßo para processamento de grandes volumes de dados usando frameworks como Apache Spark.


## üìã **Descri√ß√£o do Processo**

1. **Cria√ß√£o do Bucket S3:**
   - Crie um bucket S3 com pastas para "dados" e "jobs".
   - Carregue os dados e o script PySpark nas pastas apropriadas.

2. **Configura√ß√£o do Cluster EMR:**
   - No console do EMR, crie um cluster configurado para usar Spark.
   - Configure o cluster, incluindo VPC, logs, e permiss√µes.
   - Configure o grupo de seguran√ßa do EC2 para permitir acesso SSH.
   - Criar Cluster no menu do EMR (Aplicativos - Spark , configura√ß√£o cluster, VPC - usar o autom√°tico, encerramento do cluster, Logs de cluster - Adicionar o bucket criado para grava√ß√£o no item anterior, Configura√ß√£o de seguran√ßa e par de chaves do EC2 - Criar par de chaves tipo RSA e .ppk para Windows para liberar o acesso via m√°quina local, Perfis do Identity and Access Management (IAM) para conversa√ß√£o entre servi√ßos AWS- Criar automaticamente, Perfil de inst√¢ncia do EC2 para o Amazon EMR para liberar o acesso de leitura e escrita da instancia ao bucket S3- Criar automaticamente);
    - Ajustar no menu do cluster EMR "EC2 security groups (firewall)": Primary node > inbound role > add rule > SSH > 0.0.0.0./0 > para o terminal da m√°quina local conseguir acessar o cluster na nuvem AWS;

3. **Acesso ao Cluster EMR:**
   - Conecte-se ao cluster EMR via linha de comando (usando Putty para Windows).

    - Acessar o cluster EMR via linha de comando: Para Windows basta fazer a conex√£o com o putty.exe;

4. **Configura√ß√£o do Job PySpark:**
   - Acesse a aba "steps" do cluster EMR.
   - Adicione uma aplica√ß√£o Spark com a localiza√ß√£o do script `job.py` no bucket S3.
   - Configure os argumentos da aplica√ß√£o para especificar a URI de entrada e sa√≠da.
   - Com a conex√£o relizada, basta acessar a aba "steps" do cluster criado e adicionar um Spark application > Cluster mode > Application location (job.py) bucket S3 > Argumentos (adicionar a URI: --data_source s3://p3-dsa-dl-hk/dados/dataset.csv e --output_uri s3://p3-dsa-dl-hk/saida) em linhas separadas, pois no script Spark h√° argumentos na fun√ß√£o def); 

5. **Execu√ß√£o e Processamento:**
   - O processamento de dados ser√° realizado pelo job PySpark conforme configurado.
   - Processamento de dados com PySpark ser√° realizado ap√≥s configura√ß√µes do "steps".



## üñ•Ô∏è **Comandos:**

### Dados (dataset) (Amostra de dados):
name,inspection_result,inspection_closed_business,violation_type,violation_points
100 LB CLAM,Incomplete,FALSE,,0
100 LB CLAM,Unsatisfactory,FALSE,BLUE,5
100 LB CLAM,Unsatisfactory,FALSE,RED,5
100 LB CLAM,Unsatisfactory,FALSE,RED,10
100 LB CLAM,Unsatisfactory,FALSE,RED,5
100 LB CLAM,Complete,FALSE,,0
100 LB CLAM,Complete,FALSE,,0
100 PERCENT NUTRICION,Unsatisfactory,FALSE,BLUE,5
100 PERCENT NUTRICION,Unsatisfactory,FALSE,BLUE,5
100 PERCENT NUTRICION,Unsatisfactory,FALSE,RED,10
100 PERCENT NUTRICION,Unsatisfactory,FALSE,RED,5
1000 SPIRITS,Satisfactory,FALSE,BLUE,5
1000 SPIRITS,Satisfactory,FALSE,,0
1000 SPIRITS,Unsatisfactory,FALSE,RED,5
1000 SPIRITS,Complete,FALSE,,0
1000 SPIRITS,Satisfactory,FALSE,BLUE,5
1000 SPIRITS,Satisfactory,FALSE,,0
1000 SPIRITS,Unsatisfactory,FALSE,BLUE,5


### Job:

```python
import argparse

from pyspark.sql import SparkSession

def calculate_red_violations(data_source, output_uri):

    with SparkSession.builder.appName("Job Projeto 3 DSA").getOrCreate() as spark:
        
        # Carrega os dados
        if data_source is not None:
            restaurants_df = spark.read.option("header", "true").csv(data_source)

        # Cria um DataFrame in-memory para executar a consulta
        restaurants_df.createOrReplaceTempView("restaurant_violations")

        # Cria um DataFrame com os Top 10 Restaurantes com mais "red violations"
        top_red_violation_restaurants = spark.sql("""SELECT name, count(*) AS total_red_violations 
          FROM restaurant_violations 
          WHERE violation_type = 'RED' 
          GROUP BY name 
          ORDER BY total_red_violations DESC LIMIT 10""")

        # Grava os resultados
        top_red_violation_restaurants.write.option("header", "true").mode("overwrite").csv(output_uri)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_source', help="A URI para o arquivo de origem no Data Lake.")
    parser.add_argument('--output_uri', help="A URI para salvar o resultado.")
    args = parser.parse_args()

    calculate_red_violations(args.data_source, args.output_uri)
			
```

